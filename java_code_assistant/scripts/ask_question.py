import chromadb
from sentence_transformers import SentenceTransformer
from transformers import pipeline
from huggingface_hub import login

# âœ… Global Variables (for reuse)
embedding_model = None
chroma_client = None
collection = None
deepseek_generator = None

def initialize_models():
    """Initialize the embedding model, ChromaDB, and DeepSeek AI."""
    global embedding_model, chroma_client, collection, deepseek_generator

    print("\nğŸ”¹ Logging into Hugging Face API...")
    login("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")  # Use your actual HF token
    print("âœ… Successfully authenticated with Hugging Face.")

    print("\nğŸ”¹ Loading embedding model and connecting to ChromaDB...")
    embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
    chroma_client = chromadb.PersistentClient(path="../chroma_db")
    collection = chroma_client.get_collection(name="airline_reservation_system")
    print("âœ… Embedding model and ChromaDB loaded successfully.")

    print("\nğŸš€ Loading DeepSeek AI model...")
    deepseek_generator = pipeline(
        "text-generation",
        model="deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        trust_remote_code=True
    )
    print("âœ… DeepSeek AI model loaded successfully.")

def retrieve_answer_from_chromadb(query_text):
    """Fetch the best-matching answer from ChromaDB."""
    query_embedding = embedding_model.encode(query_text).tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=1)

    if not results["metadatas"] or len(results["metadatas"][0]) == 0:
        return "âš ï¸ No relevant answer found in ChromaDB."
    else:
        return results["metadatas"][0][0].get("answer", "âš ï¸ No answer available")

def enhance_with_deepseek(answer):
    """Improve answer clarity and completeness using DeepSeek AI."""
    print("\nğŸš€ Enhancing answer with DeepSeek AI...")
    prompt = f"Detailed Explanation answer:\n{answer}"

    refined_answer = deepseek_generator(
        prompt, max_length=2000, do_sample=True, truncation=True
    )

    if refined_answer and isinstance(refined_answer, list) and "generated_text" in refined_answer[0]:
        final_answer = refined_answer[0]["generated_text"].strip()
        return final_answer.split("</think>")[-1].strip()  # Clean unwanted output
    else:
        return "âš ï¸ DeepSeek did not return a valid response. Showing original answer."

def main():
    """Interactive Q&A loop until user types 'exit'."""
    try:
        initialize_models()  # âœ… Load everything once at startup

        while True:
            user_query = input("\nğŸ’¬ Ask a question (or type 'exit' to quit): ").strip()
            if user_query.lower() == "exit":
                print("ğŸ‘‹ Exiting the program. Have a great day!")
                break

            print(f"\nğŸ” Searching for: '{user_query}'")
            retrieved_answer = retrieve_answer_from_chromadb(user_query)

            print("\nâœ… Answer from ChromaDB:")
            print("--------")
            print(retrieved_answer)

            # Enhance with DeepSeek only if ChromaDB's answer is generic or missing
            final_answer = enhance_with_deepseek(retrieved_answer)
            print("\nâœ… Final Enhanced Answer:")
            print("--------")
            print(final_answer)

    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")

if __name__ == "__main__":
    main()




